{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Out-of-Core MOE for Multi-Task RL\n",
        "\n",
        "Training notebook for the ooc_moe project. Upload `ooc_moe.tar.gz` to run."
      ],
      "metadata": {"id": "header"}
    },
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies\n",
        "!pip install -q torch numpy gymnasium opencv-python matplotlib"
      ],
      "metadata": {"id": "install"},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload and extract the repo\n",
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "if not os.path.exists('ooc_moe'):\n",
        "    print('Upload ooc_moe.tar.gz')\n",
        "    uploaded = files.upload()\n",
        "    !tar -xzf ooc_moe.tar.gz\n",
        "    print('Extracted!')\n",
        "else:\n",
        "    print('ooc_moe already exists')"
      ],
      "metadata": {"id": "upload"},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup imports\n",
        "import sys\n",
        "sys.path.insert(0, '.')\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from collections import defaultdict, deque\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "\n",
        "from ooc_moe.models.moe_agent import MoERLAgent, MoERLAgentConfig\n",
        "from ooc_moe.envs.atari_wrappers import create_dummy_envs\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f'PyTorch {torch.__version__} on {device}')\n",
        "if device == 'cuda':\n",
        "    print(f'GPU: {torch.cuda.get_device_name(0)}')"
      ],
      "metadata": {"id": "imports"},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration\n",
        "CONFIG = {\n",
        "    'num_experts': 64,\n",
        "    'expert_dim': 128,\n",
        "    'expert_hidden': 256,\n",
        "    'num_layers': 2,\n",
        "    'num_heads': 4,\n",
        "    'top_k': 2,\n",
        "    'context_len': 8,\n",
        "    'hbm_capacity': 16,\n",
        "    'dram_capacity': 32,\n",
        "    'num_games': 5,\n",
        "    'steps_per_game': 1000,\n",
        "    'lr': 1e-4,\n",
        "}\n",
        "print('Config:', CONFIG)"
      ],
      "metadata": {"id": "config"},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create agent and environments\n",
        "config = MoERLAgentConfig(\n",
        "    obs_shape=(1, 84, 84),\n",
        "    num_actions=18,\n",
        "    num_envs=CONFIG['num_games'],\n",
        "    frame_stack=4,\n",
        "    num_experts=CONFIG['num_experts'],\n",
        "    expert_dim=CONFIG['expert_dim'],\n",
        "    expert_hidden=CONFIG['expert_hidden'],\n",
        "    num_layers=CONFIG['num_layers'],\n",
        "    num_heads=CONFIG['num_heads'],\n",
        "    top_k=CONFIG['top_k'],\n",
        "    context_len=CONFIG['context_len'],\n",
        "    hbm_cap=CONFIG['hbm_capacity'],\n",
        "    dram_cap=CONFIG['dram_capacity'],\n",
        ")\n",
        "\n",
        "agent = config.create_agent(device)\n",
        "envs, env_names = create_dummy_envs(CONFIG['num_games'])\n",
        "optimizer = torch.optim.Adam(agent.parameters(), lr=CONFIG['lr'])\n",
        "\n",
        "print(f'Agent created with {config.estimate_params()[\"total\"]:,} params')\n",
        "print(f'Environments: {env_names}')"
      ],
      "metadata": {"id": "create_agent"},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "results = {\n",
        "    'cache_history': [],\n",
        "    'expert_usage': defaultdict(lambda: defaultdict(int)),\n",
        "    'rewards': defaultdict(list),\n",
        "    'losses': [],\n",
        "}\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "for game_id, (env, name) in enumerate(zip(envs, env_names)):\n",
        "    print(f'\\nTraining on {name} ({game_id+1}/{len(envs)})...')\n",
        "    \n",
        "    obs, _ = env.reset()\n",
        "    obs_buffer = deque(\n",
        "        [torch.from_numpy(obs.astype(np.float32) / 255.0) for _ in range(config.context_len)],\n",
        "        maxlen=config.context_len\n",
        "    )\n",
        "    \n",
        "    episode_reward = 0\n",
        "    episode_count = 0\n",
        "    \n",
        "    for step in range(CONFIG['steps_per_game']):\n",
        "        # Build context\n",
        "        context = torch.stack(list(obs_buffer), dim=0).unsqueeze(0).to(device)\n",
        "        \n",
        "        # Forward pass\n",
        "        agent.train()\n",
        "        output = agent(context, env_id=game_id, prefetch=True)\n",
        "        \n",
        "        # Track metrics\n",
        "        for eid in output.expert_ids:\n",
        "            results['expert_usage'][game_id][eid] += 1\n",
        "        results['cache_history'].append(output.cache_stats['hit_rate'])\n",
        "        \n",
        "        # Sample action\n",
        "        probs = F.softmax(output.action_logits, dim=-1)\n",
        "        action = torch.multinomial(probs, 1).item()\n",
        "        \n",
        "        # Environment step\n",
        "        next_obs, reward, done, _, _ = env.step(action)\n",
        "        episode_reward += reward\n",
        "        \n",
        "        # Policy gradient update\n",
        "        log_prob = torch.log(probs[0, action] + 1e-8)\n",
        "        loss = -log_prob * reward + 0.01 * output.aux_loss\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(agent.parameters(), 0.5)\n",
        "        optimizer.step()\n",
        "        \n",
        "        results['losses'].append(loss.item())\n",
        "        \n",
        "        # Update observation buffer\n",
        "        obs_buffer.append(torch.from_numpy(next_obs.astype(np.float32) / 255.0))\n",
        "        \n",
        "        if done:\n",
        "            results['rewards'][game_id].append(episode_reward)\n",
        "            episode_reward = 0\n",
        "            episode_count += 1\n",
        "            obs, _ = env.reset()\n",
        "            obs_buffer = deque(\n",
        "                [torch.from_numpy(obs.astype(np.float32) / 255.0) for _ in range(config.context_len)],\n",
        "                maxlen=config.context_len\n",
        "            )\n",
        "    \n",
        "    # Log progress\n",
        "    recent_cache = results['cache_history'][-CONFIG['steps_per_game']:]\n",
        "    avg_reward = np.mean(results['rewards'][game_id]) if results['rewards'][game_id] else 0\n",
        "    print(f'  Episodes: {episode_count}, Avg reward: {avg_reward:.2f}')\n",
        "    print(f'  Cache hit rate: {np.mean(recent_cache):.2%}')\n",
        "    print(f'  Unique experts: {len(results[\"expert_usage\"][game_id])}')\n",
        "\n",
        "elapsed = time.time() - start_time\n",
        "print(f'\\nTraining complete in {elapsed:.1f}s')\n",
        "\n",
        "# Cleanup\n",
        "agent.expert_store.shutdown()"
      ],
      "metadata": {"id": "training"},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Results visualization\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "# Cache hit rate\n",
        "ax = axes[0, 0]\n",
        "ax.plot(results['cache_history'], alpha=0.5)\n",
        "window = 100\n",
        "smoothed = np.convolve(results['cache_history'], np.ones(window)/window, mode='valid')\n",
        "ax.plot(range(window-1, len(results['cache_history'])), smoothed, 'r-', lw=2, label=f'Smoothed (w={window})')\n",
        "ax.axhline(np.mean(results['cache_history']), color='g', linestyle='--', label=f'Mean: {np.mean(results[\"cache_history\"]):.2%}')\n",
        "for i in range(1, CONFIG['num_games']):\n",
        "    ax.axvline(i * CONFIG['steps_per_game'], color='gray', linestyle=':', alpha=0.5)\n",
        "ax.set_xlabel('Step')\n",
        "ax.set_ylabel('Cache Hit Rate')\n",
        "ax.set_title('Cache Performance Over Training')\n",
        "ax.legend()\n",
        "\n",
        "# Expert usage heatmap\n",
        "ax = axes[0, 1]\n",
        "usage_matrix = np.zeros((CONFIG['num_games'], CONFIG['num_experts']))\n",
        "for gid, usage in results['expert_usage'].items():\n",
        "    for eid, count in usage.items():\n",
        "        if eid < CONFIG['num_experts']:\n",
        "            usage_matrix[gid, eid] = count\n",
        "usage_matrix = usage_matrix / (usage_matrix.sum(axis=1, keepdims=True) + 1e-8)\n",
        "im = ax.imshow(usage_matrix, aspect='auto', cmap='hot')\n",
        "ax.set_xlabel('Expert ID')\n",
        "ax.set_ylabel('Game ID')\n",
        "ax.set_title('Expert Usage by Game (Normalized)')\n",
        "plt.colorbar(im, ax=ax)\n",
        "\n",
        "# Episode rewards\n",
        "ax = axes[1, 0]\n",
        "for gid, rewards in results['rewards'].items():\n",
        "    if rewards:\n",
        "        ax.plot(rewards, label=f'Game {gid}', alpha=0.7)\n",
        "ax.set_xlabel('Episode')\n",
        "ax.set_ylabel('Reward')\n",
        "ax.set_title('Episode Rewards')\n",
        "ax.legend()\n",
        "\n",
        "# Expert overlap\n",
        "ax = axes[1, 1]\n",
        "def get_top_k(usage_dict, k=10):\n",
        "    return set(e for e, _ in sorted(usage_dict.items(), key=lambda x: x[1], reverse=True)[:k])\n",
        "\n",
        "overlap_matrix = np.zeros((CONFIG['num_games'], CONFIG['num_games']))\n",
        "for i in range(CONFIG['num_games']):\n",
        "    for j in range(CONFIG['num_games']):\n",
        "        top_i = get_top_k(results['expert_usage'][i])\n",
        "        top_j = get_top_k(results['expert_usage'][j])\n",
        "        overlap_matrix[i, j] = len(top_i & top_j) / 10\n",
        "\n",
        "im = ax.imshow(overlap_matrix, cmap='Blues', vmin=0, vmax=1)\n",
        "ax.set_xlabel('Game')\n",
        "ax.set_ylabel('Game')\n",
        "ax.set_title('Expert Overlap (Top 10)')\n",
        "for i in range(CONFIG['num_games']):\n",
        "    for j in range(CONFIG['num_games']):\n",
        "        ax.text(j, i, f'{overlap_matrix[i,j]:.0%}', ha='center', va='center')\n",
        "plt.colorbar(im, ax=ax)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('training_results.png', dpi=150)\n",
        "plt.show()"
      ],
      "metadata": {"id": "visualize"},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Summary statistics\n",
        "print('=' * 60)\n",
        "print('TRAINING SUMMARY')\n",
        "print('=' * 60)\n",
        "\n",
        "print(f'\\nOverall cache hit rate: {np.mean(results[\"cache_history\"]):.2%}')\n",
        "print(f'Final cache hit rate: {np.mean(results[\"cache_history\"][-500:]):.2%}')\n",
        "\n",
        "print('\\nPer-game statistics:')\n",
        "for gid in range(CONFIG['num_games']):\n",
        "    usage = results['expert_usage'][gid]\n",
        "    rewards = results['rewards'][gid]\n",
        "    top5 = sorted(usage.items(), key=lambda x: x[1], reverse=True)[:5]\n",
        "    total = sum(usage.values())\n",
        "    print(f'\\n  Game {gid}:')\n",
        "    print(f'    Unique experts: {len(usage)}')\n",
        "    print(f'    Top 5 experts: {[(e, f\"{c/total:.1%}\") for e, c in top5]}')\n",
        "    print(f'    Avg episode reward: {np.mean(rewards) if rewards else 0:.2f}')\n",
        "\n",
        "print('\\nExpert specialization (low overlap = good specialization):')\n",
        "overlaps = []\n",
        "for i in range(CONFIG['num_games']):\n",
        "    for j in range(i+1, CONFIG['num_games']):\n",
        "        top_i = get_top_k(results['expert_usage'][i])\n",
        "        top_j = get_top_k(results['expert_usage'][j])\n",
        "        overlap = len(top_i & top_j)\n",
        "        overlaps.append(overlap)\n",
        "        print(f'  Game {i} vs {j}: {overlap}/10 experts overlap')\n",
        "print(f'  Average overlap: {np.mean(overlaps):.1f}/10')"
      ],
      "metadata": {"id": "summary"},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save results\n",
        "import json\n",
        "\n",
        "save_data = {\n",
        "    'config': CONFIG,\n",
        "    'cache_history': results['cache_history'],\n",
        "    'expert_usage': {int(k): dict(v) for k, v in results['expert_usage'].items()},\n",
        "    'rewards': {int(k): list(v) for k, v in results['rewards'].items()},\n",
        "    'summary': {\n",
        "        'mean_cache_hit': float(np.mean(results['cache_history'])),\n",
        "        'final_cache_hit': float(np.mean(results['cache_history'][-500:])),\n",
        "        'avg_overlap': float(np.mean(overlaps)),\n",
        "    }\n",
        "}\n",
        "\n",
        "with open('results.json', 'w') as f:\n",
        "    json.dump(save_data, f, indent=2)\n",
        "\n",
        "print('Results saved to results.json')\n",
        "files.download('results.json')\n",
        "files.download('training_results.png')"
      ],
      "metadata": {"id": "save"},
      "execution_count": null,
      "outputs": []
    }
  ]
}
